{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, output_embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, embed_dim) # each token in the vocabulary has an embedding vector\n",
    "        self.encoder = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(embed_dim, nhead=8, batch_first=True),\n",
    "            num_layers = 3,\n",
    "            norm = torch.nn.LayerNorm([embed_dim]),\n",
    "            enable_nested_tensor=False\n",
    "        )\n",
    "        self.projection = torch.nn.Linear(embed_dim, output_embed_dim)\n",
    "        \n",
    "    def forward(self, tokenizer_output):\n",
    "        x = self.embedding_layer(tokenizer_output['input_ids'])\n",
    "        x = self.encoder(x, src_key_padding_mask= tokenizer_output['attention_mask'].logical_not())\n",
    "        cls_embed = x[:,0,:]\n",
    "        return self.projection(cls_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "def train_loop(dataset, num_epochs = 1, verbose = False):\n",
    "    embed_size = 512\n",
    "    output_embed_size = 128\n",
    "    max_seq_len = 64\n",
    "    batch_size = 32\n",
    "    \n",
    "    # number of iterations\n",
    "    n_iters = (len(dataset)//batch_size) + 1\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # question encoder\n",
    "    question_encoder = Encoder(tokenizer.vocab_size, embed_size, output_embed_size)\n",
    "    answer_encoder = Encoder(tokenizer.vocab_size, embed_size, output_embed_size)\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(list(question_encoder.parameters()) + list(answer_encoder.parameters()), lr=1e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = []\n",
    "        for index, batch_data in enumerate(dataloader):\n",
    "            \n",
    "            # since out batchsize is 32, here we will get 32 question and 32 answers.\n",
    "            questions, answers = batch_data\n",
    "        \n",
    "            # tokenization\n",
    "            question_tok = tokenizer(questions, padding= True, truncation=True, return_tensors=\"pt\", max_length=max_seq_len)\n",
    "            answer_tok = tokenizer(answers, padding= True, truncation=True, return_tensors=\"pt\", max_length=max_seq_len)\n",
    "            \n",
    "            # embeddings\n",
    "            question_embd = question_encoder(question_tok) # dim = 32 x 128\n",
    "            answer_embd = answer_encoder(answer_tok)       # dim = 32 x 128\n",
    "            \n",
    "            # dot product to calculate the similarity score\n",
    "            similarity_score = question_embd @ answer_embd.T\n",
    "            \n",
    "            if index == 0 and epoch == 0:\n",
    "                print(f\"question tokens : {question_tok['input_ids'].shape}, answer tokens : {answer_tok['input_ids'].shape}\")\n",
    "                print(f\"question embedding : {question_embd.shape}, answer embedding : {answer_embd.shape}\")\n",
    "                print(f\"similarity score shape : {similarity_score.shape}\")\n",
    "            \n",
    "            target = torch.arange(question_embd.shape[0], dtype=torch.long)\n",
    "            loss = loss_fn(similarity_score, target)\n",
    "            epoch_loss += [loss.item()]\n",
    "            \n",
    "            if (index == n_iters - 1) and (verbose == True):\n",
    "                print(f\"epoch : {epoch} , loss : {np.mean(epoch_loss)}\")\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return question_encoder, answer_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.data = pd.read_csv(path, sep = '\\t')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.data.iloc[index]['questions'], self.data.iloc[index]['answers']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>who played bubba in the tv series in the heat ...</td>\n",
       "      <td>Carlos Alan Autry Jr. (also known for a period...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>where did the 2017 tour de france start</td>\n",
       "      <td>The 3,540 km (2,200 mi)-long race commenced wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>who is the chess champion of the world</td>\n",
       "      <td>Current world champion Magnus Carlsen won the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>who scored the most hat tricks in football</td>\n",
       "      <td>Cristiano Ronaldo and Messi have scored three ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           questions  \\\n",
       "0  who played bubba in the tv series in the heat ...   \n",
       "1            where did the 2017 tour de france start   \n",
       "2             who is the chess champion of the world   \n",
       "3         who scored the most hat tricks in football   \n",
       "\n",
       "                                             answers  \n",
       "0  Carlos Alan Autry Jr. (also known for a period...  \n",
       "1  The 3,540 km (2,200 mi)-long race commenced wi...  \n",
       "2  Current world champion Magnus Carlsen won the ...  \n",
       "3  Cristiano Ronaldo and Messi have scored three ...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = get_dataset('qa_dataset.tsv')\n",
    "dataset.data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'who played bubba in the tv series in the heat of the night'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data['questions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Carlos Alan Autry Jr. (also known for a period of time as Carlos Brown; born July 31, 1952), is an American actor, politician, and former National Football League player.'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data['answers'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question tokens : torch.Size([32, 14]), answer tokens : torch.Size([32, 64])\n",
      "question embedding : torch.Size([32, 128]), answer embedding : torch.Size([32, 128])\n",
      "similarity score shape : torch.Size([32, 32])\n",
      "epoch : 0 , loss : 3.736607886850834\n",
      "epoch : 1 , loss : 3.5064046904444695\n",
      "epoch : 2 , loss : 3.4198911264538765\n",
      "epoch : 3 , loss : 3.376646675169468\n",
      "epoch : 4 , loss : 3.2931443825364113\n",
      "epoch : 5 , loss : 3.228568986058235\n",
      "epoch : 6 , loss : 3.133317343890667\n",
      "epoch : 7 , loss : 3.043817799538374\n",
      "epoch : 8 , loss : 2.9490405060350895\n",
      "epoch : 9 , loss : 2.799636036157608\n"
     ]
    }
   ],
   "source": [
    "question_encoder, answer_encoder = train_loop(dataset,num_epochs=10, verbose= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4559,  0.3104,  0.1083,  0.2180,  0.6370,  0.3144,  0.0525,  0.2199,\n",
       "         0.1124, -0.0224, -0.2600,  0.4079,  0.4972,  0.3436,  0.1591,  0.3813,\n",
       "         0.1897,  0.0105,  0.0283,  0.4334,  0.3882,  0.1251,  0.5354, -0.0250,\n",
       "        -0.0951,  0.1878,  0.4431, -0.1647,  0.2198, -0.1959,  0.0997,  0.0196,\n",
       "        -0.0581, -0.1162,  0.3346, -0.1372,  0.1587,  0.2087, -0.4258,  0.1915,\n",
       "         0.3737, -0.4297, -0.1160,  0.5246, -0.2358,  0.0687,  0.3521, -0.7793,\n",
       "        -0.0573, -0.5430, -0.1427,  0.0986,  0.0726,  0.0713,  0.2144,  0.2821,\n",
       "        -0.0862,  0.4196,  0.1255,  0.2805,  0.0521, -0.5239, -0.2706,  0.3674,\n",
       "        -0.3771, -0.3281, -0.0123,  0.1145, -0.1107,  0.1231,  0.2426,  0.2001,\n",
       "         0.0658, -0.2434, -0.4554, -0.5927,  0.1864,  0.0623,  0.1784,  0.0910,\n",
       "         0.0186, -0.2464, -0.2155, -0.1771,  0.4431, -0.0783,  0.3244,  0.3044,\n",
       "         0.2192,  0.2269, -0.1557,  0.1104, -0.3379, -0.0858,  0.2079, -0.2956,\n",
       "        -0.6909, -0.3882, -0.1762, -0.0332, -0.3406, -0.0927, -0.2037, -0.4604,\n",
       "        -0.3593,  0.1123, -0.1222, -0.0014,  0.4530,  0.0208,  0.0082, -0.0381,\n",
       "         0.0118,  0.0313,  0.2329,  0.4158, -0.1788, -0.0516, -0.0916, -0.0712,\n",
       "        -0.3005, -0.0465, -0.2633, -0.1651,  0.4335,  0.0498, -0.4837, -0.4204],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who proposed the theory of general relativity?\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "question_tok = tokenizer(question, padding=True, truncation=True, return_tensors=\"pt\", max_length=64)\n",
    "question_embedding = question_encoder(question_tok)[0]\n",
    "question_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2040,  3818,  1996,  3399,  1997,  2236, 20805,  1029,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers =[\n",
    "    \"General Relativity was established in 1915 by Albert Einstein\",\n",
    "    \"The sum of 1 + 2 is equal to 3\",\n",
    "    \"Who proposed the theory of general relativity?\"\n",
    "]\n",
    "\n",
    "ans_tokens = []\n",
    "ans_embeddings = []\n",
    "similary_scores = []\n",
    "for answer in answers:\n",
    "    answer_tok = tokenizer(answer, padding=True, truncation=True, return_tensors=\"pt\", max_length=64)\n",
    "    ans_tokens.append(answer_tok)\n",
    "    answer_embedding = answer_encoder(answer_tok)[0]\n",
    "    ans_embeddings.append(answer_embedding)\n",
    "    similarity_score = question_embedding @ answer_embedding.T\n",
    "    similary_scores.append(similarity_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5183, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embedding@answer_embedding.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.9853, grad_fn=<DotBackward0>),\n",
       " tensor(1.2378, grad_fn=<DotBackward0>),\n",
       " tensor(1.5183, grad_fn=<DotBackward0>)]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similary_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[  101,  2236, 20805,  2001,  2511,  1999,  4936,  2011,  4789, 15313,\n",
       "            102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[ 101, 1996, 7680, 1997, 1015, 1009, 1016, 2003, 5020, 2000, 1017,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[  101,  2040,  3818,  1996,  3399,  1997,  2236, 20805,  1029,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  2236, 20805,  2001,  2511,  1999,  4936,  2011,  4789, 15313,\n",
      "          102])\n",
      "tensor([ 101, 1996, 7680, 1997, 1015, 1009, 1016, 2003, 5020, 2000, 1017,  102])\n",
      "tensor([  101,  2040,  3818,  1996,  3399,  1997,  2236, 20805,  1029,   102])\n"
     ]
    }
   ],
   "source": [
    "for token in ans_tokens:\n",
    "    print(token['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_embeddings[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question embedding : tensor([-0.4559,  0.3104,  0.1083,  0.2180,  0.6370,  0.3144,  0.0525,  0.2199,\n",
      "         0.1124, -0.0224], grad_fn=<SliceBackward0>)\n",
      "answer embedding :\n",
      "tensor([ 0.0824,  0.4870, -0.1635,  0.7219,  0.3285, -1.0932,  0.3275, -0.0585,\n",
      "        -0.0921, -0.3693], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.5028,  0.7946, -0.7614,  0.3862,  0.6956, -1.0700,  0.5501,  0.0275,\n",
      "         0.0091, -0.1320], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.1641,  0.3631, -0.3865,  0.2011,  0.6216, -0.5416,  0.4523,  0.1810,\n",
      "         0.3832, -0.2904], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"question embedding : {question_embedding[:10]}\")\n",
    "print(\"answer embedding :\")\n",
    "for embedding in ans_embeddings:\n",
    "    print(embedding[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
