{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        [0.3, 1.2, 0.05, 1.07],\n",
    "        [4.18, 3.2, 0.09, 0.05],\n",
    "        [0.85, 0.27, 2.2, 5.03],\n",
    "        [0.23, 0.57, 6.12, 5.1]\n",
    "    ]\n",
    ")\n",
    "data = torch.tensor(df.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3 , 1.2 , 0.05, 1.07],\n",
       "       [4.18, 3.2 , 0.09, 0.05],\n",
       "       [0.85, 0.27, 2.2 , 5.03],\n",
       "       [0.23, 0.57, 6.12, 5.1 ]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(data):\n",
    "    target = torch.arange(data.size(0))\n",
    "    loss = torch.nn.CrossEntropyLoss()(data, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = data.size(0)\n",
    "non_diag_mask = ~torch.eye(N, N, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2000, 0.0500, 1.0700, 0.1800, 0.0900, 0.0500, 0.8500, 0.2700, 1.0300,\n",
       "        0.2300, 0.5700, 0.1200])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[non_diag_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8000, 1.1800, 0.0300, 1.0500],\n",
      "        [4.1600, 3.7000, 0.0700, 0.0300],\n",
      "        [0.8300, 0.2500, 2.7000, 5.0100],\n",
      "        [0.2100, 0.5500, 6.1000, 5.6000]])\n",
      "Loss = 1.4527289867401123\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(df.values, dtype=torch.float32)\n",
    "data[range(N), range(N)] += 0.5\n",
    "data[non_diag_mask] -= 0.02\n",
    "print(data)\n",
    "print(f\"Loss = {contrastive_loss(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1563, 0.3844, 0.1217, 0.3376],\n",
       "        [0.7102, 0.2665, 0.0119, 0.0114],\n",
       "        [0.0141, 0.0079, 0.0545, 0.9235],\n",
       "        [0.0020, 0.0028, 0.7314, 0.2637]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Softmax(dim=1)(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Softmax(dim=1)(data).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3000, 1.2000, 0.0500, 1.0700],\n",
      "        [4.1800, 3.2000, 0.0900, 0.0500],\n",
      "        [0.8500, 0.2700, 2.2000, 5.0300],\n",
      "        [0.2300, 0.5700, 6.1200, 5.1000]])\n",
      "Loss = 1.8551747798919678\n",
      "tensor([[0.8000, 1.1800, 0.0300, 1.0500],\n",
      "        [4.1600, 3.7000, 0.0700, 0.0300],\n",
      "        [0.8300, 0.2500, 2.7000, 5.0100],\n",
      "        [0.2100, 0.5500, 6.1000, 5.6000]])\n",
      "Loss = 1.4527289867401123\n",
      "tensor([[1.3000, 1.1600, 0.0100, 1.0300],\n",
      "        [4.1400, 4.2000, 0.0500, 0.0100],\n",
      "        [0.8100, 0.2300, 3.2000, 4.9900],\n",
      "        [0.1900, 0.5300, 6.0800, 6.1000]])\n",
      "Loss = 1.0994977951049805\n",
      "tensor([[ 1.8000,  1.1400, -0.0100,  1.0100],\n",
      "        [ 4.1200,  4.7000,  0.0300, -0.0100],\n",
      "        [ 0.7900,  0.2100,  3.7000,  4.9700],\n",
      "        [ 0.1700,  0.5100,  6.0600,  6.6000]])\n",
      "Loss = 0.8030187487602234\n",
      "tensor([[ 2.3000,  1.1200, -0.0300,  0.9900],\n",
      "        [ 4.1000,  5.2000,  0.0100, -0.0300],\n",
      "        [ 0.7700,  0.1900,  4.2000,  4.9500],\n",
      "        [ 0.1500,  0.4900,  6.0400,  7.1000]])\n",
      "Loss = 0.5657716989517212\n"
     ]
    }
   ],
   "source": [
    "N = data.size(0)\n",
    "non_diag_mask = ~torch.eye(N, N, dtype=bool)\n",
    "\n",
    "for inx in range(5):\n",
    "    data = torch.tensor(df.values, dtype=torch.float32)\n",
    "    data[range(N), range(N)] += inx*0.5\n",
    "    data[non_diag_mask] -= inx*0.02\n",
    "    print(data)\n",
    "    print(f\"Loss = {contrastive_loss(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Softmax(dim=1)(data).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, output_embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(embed_dim, nhead=8, batch_first=True),\n",
    "            num_layers=3,\n",
    "            norm=torch.nn.LayerNorm([embed_dim]),\n",
    "            enable_nested_tensor=False\n",
    "        )\n",
    "        self.projection = torch.nn.Linear(embed_dim, output_embed_dim)\n",
    "    \n",
    "    def forward(self, tokenizer_output):\n",
    "        x = self.embedding_layer(tokenizer_output['input_ids'])\n",
    "        x = self.encoder(x, src_key_padding_mask=tokenizer_output['attention_mask'].logical_not())\n",
    "        cls_embed = x[:,0,:]\n",
    "        return self.projection(cls_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataset):\n",
    "    embed_size = 512\n",
    "    output_embed_size = 128\n",
    "    max_seq_len = 64\n",
    "    batch_size = 32\n",
    "\n",
    "    # define the question/answer encoders\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    question_encoder = Encoder(tokenizer.vocab_size, embed_size, \n",
    "                               output_embed_size)\n",
    "    answer_encoder = Encoder(tokenizer.vocab_size, embed_size, \n",
    "                             output_embed_size)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                             shuffle=True)    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(question_encoder.parameters()) + list(answer_encoder.parameters()\n",
    "    ), lr=1e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    running_loss = []\n",
    "    for _, data_batch in enumerate(dataloader):\n",
    "\n",
    "        # Tokenize the question/answer pairs (each is a batch of 32 questions and 32 answers)\n",
    "        question, answer = data_batch\n",
    "        question_tok = tokenizer(question, padding=True, truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "        answer_tok = tokenizer(answer, padding=True, truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "\n",
    "        # Compute the embeddings: the output is of dim = 32 x 128\n",
    "        question_embed = question_encoder(question_tok)\n",
    "        answer_embed = answer_encoder(answer_tok)\n",
    "\n",
    "        # Compute similarity scores: a 32x32 matrix\n",
    "        # row[N] reflects similarity between question[N] and answers[0...31]\n",
    "        similarity_scores = question_embed @ answer_embed.T\n",
    "\n",
    "        # we want to maximize the values in the diagonal\n",
    "        target = torch.arange(question_embed.shape[0], dtype=torch.long)\n",
    "        loss = loss_fn(similarity_scores, target)\n",
    "        running_loss += [loss.item()]\n",
    "\n",
    "        # this is where the magic happens\n",
    "        optimizer.zero_grad()    # reset optimizer so gradients are all-zero\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return question_encoder, answer_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, num_epochs=1):\n",
    "    embed_size = 512\n",
    "    output_embed_size = 128\n",
    "    max_seq_len = 64\n",
    "    batch_size = 32\n",
    "\n",
    "    n_iters = len(dataset) // batch_size + 1\n",
    "    \n",
    "    # define the question/answer encoders\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    question_encoder = Encoder(tokenizer.vocab_size, embed_size, output_embed_size)\n",
    "    answer_encoder = Encoder(tokenizer.vocab_size, embed_size, output_embed_size)\n",
    "\n",
    "    # define the dataloader, optimizer and loss function    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)    \n",
    "    optimizer = torch.optim.Adam(list(question_encoder.parameters()) + list(answer_encoder.parameters()), lr=1e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = []\n",
    "        for idx, data_batch in enumerate(dataloader):\n",
    "\n",
    "            # Tokenize the question/answer pairs (each is a batc of 32 questions and 32 answers)\n",
    "            question, answer = data_batch\n",
    "            question_tok = tokenizer(question, padding=True, truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "            answer_tok = tokenizer(answer, padding=True, truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "            if inx == 0 and epoch == 0:\n",
    "                print(question_tok['input_ids'].shape, answer_tok['input_ids'].shape)\n",
    "            \n",
    "            # Compute the embeddings: the output is of dim = 32 x 128\n",
    "            question_embed = question_encoder(question_tok)\n",
    "            answer_embed = answer_encoder(answer_tok)\n",
    "            if inx == 0 and epoch == 0:\n",
    "                print(question_embed.shape, answer_embed.shape)\n",
    "    \n",
    "            # Compute similarity scores: a 32x32 matrix\n",
    "            # row[N] reflects similarity between question[N] and answers[0...31]\n",
    "            similarity_scores = question_embed @ answer_embed.T\n",
    "            if inx == 0 and epoch == 0:\n",
    "                print(similarity_scores.shape)\n",
    "    \n",
    "            # we want to maximize the values in the diagonal\n",
    "            target = torch.arange(question_embed.shape[0], dtype=torch.long)\n",
    "            loss = loss_fn(similarity_scores, target)\n",
    "            running_loss += [loss.item()]\n",
    "            if idx == n_iters-1:\n",
    "                print(f\"Epoch {epoch}, loss = \", np.mean(running_loss))\n",
    "    \n",
    "            # this is where the magic happens\n",
    "            optimizer.zero_grad()    # reset optimizer so gradients are all-zero\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return question_encoder, answer_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>who played bubba in the tv series in the heat ...</td>\n",
       "      <td>Carlos Alan Autry Jr. (also known for a period...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>where did the 2017 tour de france start</td>\n",
       "      <td>The 3,540 km (2,200 mi)-long race commenced wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>who is the chess champion of the world</td>\n",
       "      <td>Current world champion Magnus Carlsen won the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>who scored the most hat tricks in football</td>\n",
       "      <td>Cristiano Ronaldo and Messi have scored three ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what do you need to be an ontario scholar</td>\n",
       "      <td>Ontario Scholars are high school graduates in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           questions  \\\n",
       "0  who played bubba in the tv series in the heat ...   \n",
       "1            where did the 2017 tour de france start   \n",
       "2             who is the chess champion of the world   \n",
       "3         who scored the most hat tricks in football   \n",
       "4          what do you need to be an ontario scholar   \n",
       "\n",
       "                                             answers  \n",
       "0  Carlos Alan Autry Jr. (also known for a period...  \n",
       "1  The 3,540 km (2,200 mi)-long race commenced wi...  \n",
       "2  Current world champion Magnus Carlsen won the ...  \n",
       "3  Cristiano Ronaldo and Messi have scored three ...  \n",
       "4  Ontario Scholars are high school graduates in ...  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datapath):\n",
    "        self.data = pd.read_csv(datapath, sep=\"\\t\", nrows=300)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.iloc[idx]['questions'], self.data.iloc[idx]['answers']\n",
    "\n",
    "dataset = MyDataset('qa_dataset.tsv')\n",
    "dataset.data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qe, ae = train(dataset, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2054,  2003,  1996, 13747,  3137,  1999,  1996,  2088,  1029,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([-0.1229,  0.4092,  0.4142, -0.0284,  0.0784], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "question = 'What is the tallest mountain in the world?'\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "question_tok = tokenizer(question, padding=True, truncation=True, return_tensors='pt', max_length=64)\n",
    "question_emb = qe(question_tok)[0]\n",
    "print(question_tok)\n",
    "print(question_emb[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  101,  2054,  2003,  1996, 13747,  3137,  1999,  1996,  2088,  1029,\n",
      "           102]]), tensor([[  101,  1996, 13747,  3137,  1999,  1996,  2088,  2003,  4057, 23914,\n",
      "          1012,   102]]), tensor([[ 101, 2040, 2003, 6221, 9457, 1029,  102]])]\n",
      "tensor([-0.5402,  0.8633, -0.2744, -0.2967, -0.2044], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.8422,  0.7184, -0.1301, -0.4388, -0.2017], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.6134,  0.3070,  0.1103, -0.3892, -0.5599], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "answers = [\n",
    "    \"What is the tallest mountain in the world?\",\n",
    "    \"The tallest mountain in the world is Mount Everest.\",\n",
    "    \"Who is donald duck?\"\n",
    "]\n",
    "answer_tok = []\n",
    "answer_emb = []\n",
    "for answer in answers:\n",
    "    tok = tokenizer(answer, padding=True, truncation=True, return_tensors='pt', max_length=64)\n",
    "    answer_tok.append(tok['input_ids'])\n",
    "    emb = ae(tok)[0]\n",
    "    answer_emb.append(emb)\n",
    "\n",
    "print(answer_tok)\n",
    "print(answer_emb[0][:5])\n",
    "print(answer_emb[1][:5])\n",
    "print(answer_emb[2][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0004, 1.0827, 0.0563], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_emb @ torch.stack(answer_emb).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
